
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="Vaish is an Applied Scientist at Microsoft Search Assistant and Intelligence (MSAI). Her research interests are in Machine Learning and NLP.">
  <meta name="keywords" content="Vaish Shrivastava, Machine Learning, NLP, Microsoft">
  <link rel="icon" href="images/microsoft.png">

  <title>Vaish Shrivastava</title>

  <!-- Bootstrap core CSS -->
  <link href="https://getbootstrap.com/docs/3.3/dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <link href="https://getbootstrap.com/docs/3.3/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="navbar-fixed-top.css" rel="stylesheet">

  <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
  <script src="https://getbootstrap.com/docs/3.3/assets/js/ie-emulation-modes-warning.js"></script>

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-57880796-2', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>

  <!-- Fixed navbar -->
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">Vaish Shrivastava</a>
      </div>
      <div id="navbar" class="navbar-collapse collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="#">About</a></li>
          <li><a href="#papers">Papers</a></li>
          <li><a href="#cv">CV</a></li>
          <li><a href="#projects">Recent Projects</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>


    <div class="container">
        <div class="well">
        <table style="border-spacing: 15px;border-collapse: separate;">
          <tbody>
            <tr>
              <td valign="top" margin-top="10px" style="font-size: 0pt;">
              <span style="font-size: 15px">
              <p>
              </br>
              </br>
                Hi!
              </br>
              </br>
              I'm <B>Vaish Shrivastava</B>, an <B>Applied Scientist</B> at <B><a href=https://www.microsoft.com/en-us/research/group/msai/>Microsoft Search, Assistant and Intelligence</a></B>, where I work on developing large-scale Machine Learning systems deployed to 
              millions of users! I also collaborate with <B><a href=https://www.microsoft.com/en-us/research/research-area/artificial-intelligence/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13556&sort_by=most-recent>Microsoft Research</a></B>.
              </br>
              </br> 
              My passion is bringing the benefits of Machine Learning (ML) and Natural Language Processing (NLP) to as many people as possible. I'm interested in a range of NLP subfields, 
              including Transformer-based language models and pre-trained representations, dialog systems, question answering, abstractive summarization, and multi-lingual models. 
              
            </br>
            </br>
            As an Applied Scientist at Microsoft, I have worked on several different areas in ML and NLP including <B>Transformer model compression</B>, <B>personalizing language models</B>, 
            <B>modeling multi-turn dialogs</B>, <B>factual consistency for abstractive summarization</B>, and <B>federated learning</B>.
              <hr style="width: 100%; color: black; height: 1px; background-color:black;" />
                Before joining Microsoft full-time, I went to <B><a href=https://www.caltech.edu/>Caltech</a></B> for my Bachelor's in Computer Science, and enjoyed working on multi-task reinforcement learning.
              </p>
              </span>
              <p>
              <span style="font-size: 20px">

              </span>
              </p>
              </td>
              <td width="0pct" valign="top">
              </br>
              </br>
                <div><span class="lfloat"><img width="155px" src="images/vaish4.jpg" alt="" title="Vaishnavi Shrivastava"></span></div>
              <hr style="width: 100%; color: black; height: 1px; background-color:black;" />
                <a href="https://github.com/vshrivas"><img width="25px" src="images/github_favicon.ico" alt="vshrivas" title="Github"></a>
                <a href="mailto:vashri@microsoft.com"><img width="25px" src="images/email.png" alt="vashri@microsoft.com" title="Email"> </a>
                <a href="https://www.linkedin.com/in/vaish-shrivastava/"> <img width="25px" src="images/linkedin_favicon.ico" alt="vaish-shrivastava" title="Linkedin"></a>
                <!--<a href="https://www.facebook.com/vaish.shrivastava"> <img width="20px" src="images/facebook_favicon.png" alt="vaish.shrivastava" title="Facebook"></a>-->
                <a href="https://scholar.google.com/citations?user=N0nX2VsAAAAJ&hl=en"> <img width="25px" src="images/google-scholar-icon.png" alt="vaish.shrivastava" title="Google Scholar"></a>
              </td>
            </tr>
          </tbody>
        </table>
        </div>
      </br>

      <div class="well" id="papers" style="font-size: 15px">
        <!--<table style="border-spacing: 15px;border-collapse: separate;">
          <tbody>
            <tr>
              <td valign="top" margin-top="0px" style="font-size: 0pt;">
              <span style="font-size: 15px">
              <p>-->

              <h3 style="margin-top: 0em">Papers</h3>
        <hr style="margin-top: 0em; width: 100%; color: black; height: 1px; background-color:black;" />

        <ul>
          <li>
            <b>Vaishnavi Shrivastava*</b>, Radhika Gaonkar*, Shashank Gupta*, Abhishek Jha. 2021. <i>Exploring Low-Cost Transformer Model Compression for Large-Scale Commercial Reply Suggestions.</i> <i><a href=https://arxiv.org/abs/2111.13999>arXiv: 2111.13999</a></i>
          </li>
        </br>
          <li>
            <a href=https://cseweb.ucsd.edu/~fmireshg/>Fatemehsadat Mireshghallah</a>, <b>Vaishnavi Shrivastava</b>, <a href=https://scholar.google.com/citations?hl=en&user=SbYANgwAAAAJ&view_op=list_works&sortby=pubdate>Milad Shokouhi</a>, 
            <a href= https://cseweb.ucsd.edu//~tberg/>Taylor Berg-Kirkpatrick</a>, <a href=https://www.microsoft.com/en-us/research/people/rsim/>Robert Sim</a>,
            <a href=https://www.microsoft.com/en-us/research/people/didimit/>Dimitrios Dimitriadis</a>. 2021. <i>UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis.</i> <i><a href=https://arxiv.org/abs/2110.00135>arXiv: 2110.00135</a></i> 
          </li>
        </ul>
              <!--</p>
              </span>
              <p>
              <span style="font-size: 20px">

              </span>
              </p>
              </td>
              <td width="0pct" valign="top">
              </br>
              </br>
                <div><span class="lfloat"><img width="370px" src="images/vaish3.jpg" alt="" title="Vaishnavi Shrivastava"></span></div>
              <hr style="width: 100%; color: black; height: 1px; background-color:black;" />
                <a href="https://github.com/vshrivas"><img width="25px" src="images/github_favicon.ico" alt="vshrivas" title="Github"></a>
                <a href="mailto:vashri@microsoft.com"><img width="25px" src="images/email.png" alt="vashri@microsoft.com" title="Email"> </a>
                <a href="https://www.linkedin.com/in/vaish-shrivastava/"> <img width="25px" src="images/linkedin_favicon.ico" alt="vaish-shrivastava" title="Linkedin"></a>
                <a href="https://www.facebook.com/vaish.shrivastava"> <img width="20px" src="images/facebook_favicon.png" alt="vaish.shrivastava" title="Facebook"></a>
                <a href="https://scholar.google.com/citations?user=N0nX2VsAAAAJ&hl=en"> <img width="25px" src="images/google-scholar-icon.png" alt="vaish.shrivastava" title="Google Scholar"></a>-->
              </td>
            </tr>
          </tbody>
        </table>
        </div>
      </br>

      <div class="well" id="cv" style="font-size: 15px">
        <h3 id="cv" style="margin-top: 0em">CV</h3>
        <hr style="margin-top: 0em; width: 100%; color: black; height: 1px; background-color:black;" />
        For more information, here is my <B><a href="cv/Vaishnavi_Shrivastava_CV.pdf">CV</a></B>.
      </div>

    </br>


        <div class="well" id="projects" style="font-size: 15px">
          
          <h3 style="margin-top: 0em">Recent Projects</h3>
            
          <hr style="margin-top: 0em; width: 100%; color: black; height: 1px; background-color:black;" />
            <h4 id="dataless">Personalizing Language Models </h4>
            <ul>
              Personalization is an important area for improving user experiences with language models, by learning about user preferences and tailoring model outputs accordingly.
              In the case of user-personalization, each user can be treated as a separate 'downstream task' for our language model. Fine-tuning large Transformer-based language models is currently
              the most popular technique for adapting these models to downstream tasks. But, given the massive number of users in our scenarios, fully fine-tuning separate models for each
              user would be infeasible, both in terms of training time and inferencing latencies. Thus my work explores light-weight fine-tuning techniques for these language models, to adapt them to user-specific behavior.
            </br>
          </br>

            <u><h4 id="one-shot">Prefix-Tuning with User Signals</u> <i><span style="font-weight:normal;margin-left:0.25em;font-size: 15px">(July 2021 - Present)</span></i><span style="margin-left:0.50em;font-size: 15px"> </h4>
              Large language models can store a great deal of information within their parameters, and as recently demonstrated by zero-shot and few-shot learning in models like GPT-3,
              prompts can be used to induce particular model behavior. Prompt engineering was the strategy of crafting these prompts to trigger task-specific behavior by more overtly communicating the
              task with the model. </br></br>
              Prefix-tuning has recently emerged as a means of finding more optimal prompts over a continuous space to condition the model at the embedding or activation levels. Conditioning
              models at the embedding layer would entail generating associated keys and values from the model and then passing these in as past key values alongside inputs, to condition model outputs.
              Meanwhile, prefix-tuning at the activation stage would allow for more fine-grained control over personalization by directly generating key value activations, instead of allowing the model
              to generate them through embeddings. Optimizing over this large space of key value activations could allow us to find more optimal ways of conditioning the model for specific tasks.
              In the context of user-personalization, users are the tasks we wish to optimize for. 
              </br></br>
              <ul>
                <li>
                  Adapting prefix-tuning to the task of user-personalization, we train user-specific prompts to induce personalized response generation from a GPT-2 based model, at the embedding and activation levels.
                </li>
                <li>
                  To allow this model to personalize responses for users not in the training set, we further ground this model in user language signals. 
                </li>
                <li>
                  N-gram signals from user mailboxes are used to generate embeddings jointly trained with the GPT-2 to condition personalized responses for users not seen in the training set.
                </li>
                <li>
                  Next we will be training this model with differentially private algorithms such as differentially private stochastic gradient descent (DP-SGD) to preserve user-privacy.
                </li>
              </ul>

            </br>
            <u><h4 id="one-shot">Fixed Prompts for Implicit Personalized User Representations</u> <i><span style="font-weight:normal;margin-left:0.25em;font-size: 15px">(July 2021 - September 2021)</span></i><span style="margin-left:0.50em;font-size: 15px"> </h4>
              Training and updating embeddings for millions of users can still be incredibly expensive, especially given data refreshes to personalize content based on a user's current writing style.
              Thus a natural next question to explore was if instead of learning per-user embeddings, we could simply use fixed prompts to retrieve user-specific information from within a model. </br></br>
              <ul>
                <li>
                  Recently prompting of models like GPT-3 has led to impressive results in zero-shot and few-shot settings. In a similar vein, we explored appending fixed sequences called 
                  user-identifiers to model inputs to prompt the model to produce personalized, user-specific outputs.
                </li>
                <li>
                  We also conducted a series of ablation experiments, investigating the types of fixed prompts that would be most performant, including the length of these prompts and the distribution
                  the prompt tokens are drawn from.
                </li>
                <li>
                  Demonstrated up to 13% improvement over prefix-tuning based SOTA on a suite of sentiment analysis tasks.
                </li>
              </ul>


            </br>
            <u><h4 id="one-shot">Low-Rank Adaptation of Large Language Models</u> <i><span style="font-weight:normal;margin-left:0.25em;font-size: 15px">(November 2021 - Present)</span></i><span style="margin-left:0.50em;font-size: 15px"> </h4>
              Recent works show that large language models exist on a low intrinsic dimension. The work on Low-Rank Adaptation of Large Language Models (LoRA) extends this argument to propose that even
              the weight updates to these models may exist in a low-rank dimension. Therefore the LoRA technique freezes the parameters of pre-trained models and represents the weight updates to these parameters with a
              product BA, where B has dimensions d x r and A has dimensions r x k, where r is the low-rank dimension. </br></br>
              While other parameter-efficient tuning techniques exist, such as the prefix-tuning method mentioned above and adding adapter layers, these methods come with their own issues. 
              Adapter layers can increase online inferencing latencies, while prefix-tuning consumes a portion of the sequence length available to downstream tasks. </br></br>
              In our case, we adapt LoRA to the scenario of user-personalization. 
              <ul>
                <li>
                  LoRA is used to enable more fine-grained control of personalization, by directly personalizing the weight updates to key, query, value attention matrices within the GPT-2 model.
                </li>
                <li>
                  We project user-specific embeddings into the space of rank-decomposed weight updates to attention matrices which then augment the frozen parameters of the pre-trained model.
                </li>
              </ul>
   
            </ul>
          </br>
          <!--<h4 id="dataless"> Compressing Transformer-based Language Models </h4>


          <h4 id="dataless"> Contextualization </h4>
          <h4 id="dataless"> Abstractive Summarization </h4>-->
          </div>

</br>

<!--<div class="well" id="cv" style="font-size: 15px">
        <h3 id="cv" style="margin-top: 0em">CV</h3>
        <hr style="margin-top: 0em; width: 100%; color: black; height: 1px; background-color:black;" />
        Here's my <a href="cv/Vaishnavi_Shrivastava_CV.pdf">CV</a>.
</div>

</br>

<div class="well" id="contact" style="font-size: 15px">
        <h3 id="contact" style="margin-top: 0em">Contact</h3>
        <hr style="margin-top: 0em; width: 100%; color: black; height: 1px; background-color:black;" />
        <ul>
          <li>
          <b>Email</b>: vashri@microsoft.com
        </li>
        <li>
          <b>Cell:</b> (+1) 408-477-5322
        </li>
        <li>
          <a href="https://scholar.google.com/citations?user=N0nX2VsAAAAJ&hl=en">Google Scholar</a>,
          <a href="https://github.com/vshrivas">Github</a>,
          <a href="https://www.linkedin.com/in/vaish-shrivastava/">Linkedin</a>
        </li>
      </ul>
</div>

</br>-->



<div id="footer">
      <span>I have used <a href="http://getbootstrap.com"> Bootstrap</a> to build this.</span>
</div>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
